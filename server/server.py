# íŒŒì¼ëª…: server.py
# ============================================================
# Personal Healthcare Assistant - Backend Server (Final Fixed)
# ============================================================

import os
from pathlib import Path
from dotenv import load_dotenv
import glob
import pandas as pd
from typing import Optional, List, Dict, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import google.generativeai as genai
import uvicorn
import traceback
import re

# ==========================================
# .env ë¡œë“œ (ê²½ë¡œ ê³ ì •)
# ==========================================
BASE_DIR = Path(__file__).resolve().parent
ROOT_DIR = BASE_DIR.parent
ENV_PATH = ROOT_DIR / ".env"

load_dotenv(dotenv_path=ENV_PATH, override=True)

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise RuntimeError("GOOGLE_API_KEY ë¡œë“œ ì‹¤íŒ¨")

# ==========================================
# Gemini ëª¨ë¸ ì„¤ì •
# ==========================================
genai.configure(api_key=GOOGLE_API_KEY)

try:
    TEXT_MODEL = genai.GenerativeModel("models/gemini-2.5-flash")
    VISION_MODEL = genai.GenerativeModel("models/gemini-2.5-flash-image")
    print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (2.5-flash)")
except Exception as e:
    print("âŒ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
    traceback.print_exc()
    TEXT_MODEL = None
    VISION_MODEL = None


# ==========================================
# ë°ì´í„° ê²½ë¡œ ì„¤ì •
# ==========================================
DATA_DIR = os.environ.get("DATA_DIR", r"C:/Users/gaeun/Desktop")
LOGS_DIR = os.path.join(DATA_DIR, "logs")
CAPTURES_DIR = os.path.join(DATA_DIR, "captures")

app = FastAPI()

# 422 ì—ëŸ¬ ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ í•¸ë“¤ëŸ¬
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    print(f"âŒ ë°ì´í„° ê²€ì¦ ì—ëŸ¬ ë°œìƒ: {exc}")
    return JSONResponse(status_code=422, content={"detail": exc.errors()})

app.add_middleware(
    CORSMIDDLEWARE := CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if os.path.exists(CAPTURES_DIR):
    app.mount("/captures", StaticFiles(directory=CAPTURES_DIR), name="captures")


@app.get("/")
def read_root():
    return {"status": "Server running", "data_path": DATA_DIR}


# ==========================================
# Pydantic ëª¨ë¸
# ==========================================
class AnalysisRequest(BaseModel):
    image_filename: str
    log_id: int

class BookInfo(BaseModel):
    title: str = ""
    authors: List[str] = []
    readPages: int = 0
    totalPages: int = 0
    durationMin: float = 0.0
    description: str = ""
    purpose: str = "study"

class LaptopInfo(BaseModel):
    category: str = "lecture"
    durationMin: float = 0.0
    isStudy: bool = True

class SummaryRequest(BaseModel):
    date: str
    waterMl: float
    waterGoal: float
    studyMin: float
    studyGoal: float
    bookInfo: Optional[BookInfo] = None
    laptopInfo: Optional[LaptopInfo] = None

# í”„ë¡ íŠ¸ì—”ë“œ useStudyLogs.jsì™€ ì¼ì¹˜í•˜ëŠ” ëª¨ë¸
class LogUpdateRequest(BaseModel):
    source_file: str            # í”„ë¡ íŠ¸ì—”ë“œê°€ ë³´ë‚´ëŠ” íŒŒì¼ëª…
    log_id: int | str           # ë¡œê·¸ ID (0ì´ë©´ ì „ì²´ ìˆ˜ì •)
    updates: Dict[str, Any]     # ë³€ê²½í•  ë°ì´í„° { "book_title": "...", ... }

# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================================
def get_csv_files_for_date(prefix: str, date_str: str) -> list:
    pattern = os.path.join(LOGS_DIR, f"{prefix}*{date_str}*.csv")
    return list(set(glob.glob(pattern)))

def parse_timestamp_from_filename(filename: str) -> Optional[str]:
    basename = os.path.basename(filename).replace('.csv', '')
    match1 = re.search(r'(\d{4}-\d{2}-\d{2})-(\d{2})-(\d{2})', basename)
    if match1:
        return f"{match1.group(1)}T{match1.group(2)}:{match1.group(3)}:00"
    match2 = re.search(r'(\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})', basename)
    if match2:
        return f"{match2.group(1)}T{match2.group(2).replace('-', ':')}"
    return None

def merge_csv_files(files: list) -> pd.DataFrame:
    if not files:
        return pd.DataFrame()
    dfs = []
    for f in files:
        try:
            df = pd.read_csv(f)
            if not df.empty:
                # ê° ì›ë³¸ CSV ì•ˆì—ì„œì˜ ë¡œìš° ì¸ë±ìŠ¤ë¥¼ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ìœ ì§€
                # ì—¬ëŸ¬ íŒŒì¼ì„ ë¨¸ì§€í•œ ë’¤ì—ë„ íŒŒì¼ ë‚´ë¶€ í–‰ ìœ„ì¹˜ë¥¼ ì•Œê¸° ìœ„í•´ ì‚¬ìš©
                if 'row_index' not in df.columns:
                    df['row_index'] = range(len(df))

                file_ts = parse_timestamp_from_filename(f)
                if file_ts and 'timestamp' not in df.columns:
                    df['timestamp'] = file_ts
                df['source_file'] = os.path.basename(f)
                dfs.append(df)
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {f} / {e}")
            traceback.print_exc()
            continue
    if not dfs:
        return pd.DataFrame()
    
    merged = pd.concat(dfs, ignore_index=True)
    if 'timestamp' in merged.columns:
        merged = merged.sort_values('timestamp').reset_index(drop=True)
    return merged

def format_capture_url(path):
    if pd.isna(path) or str(path).lower() == 'nan' or 'Started' in str(path):
        return None
    return f"http://localhost:8000/captures/{os.path.basename(str(path))}"

def calculate_study_duration_per_file(df: pd.DataFrame, obj_type: str):
    if 'object' not in df.columns or 'source_file' not in df.columns:
        return 0, 0, []

    type_df = df[df['object'].astype(str).str.lower() == obj_type.lower()]
    if type_df.empty or 'timestamp' not in type_df.columns:
        return 0, 0, []

    total_min = 0
    sessions = []

    for source_file, group in type_df.groupby('source_file'):
        try:
            timestamps = group['timestamp'].dropna().tolist()
            if len(timestamps) < 1:
                continue
            first = pd.to_datetime(timestamps[0])
            last = pd.to_datetime(timestamps[-1])
            duration = max(int((last - first).total_seconds() / 60), 1)
            total_min += duration

            sessions.append({
                'source_file': source_file,
                'start_time': first.strftime('%H:%M'),
                'end_time': last.strftime('%H:%M'),
                'duration_min': duration,
                'log_count': len(timestamps),
            })
        except Exception as e:
            print(f"âŒ duration ê³„ì‚° ì‹¤íŒ¨ ({source_file}): {e}")
            traceback.print_exc()
            continue

    return total_min, len(sessions), sessions


# ==========================================
# API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/api/logs/water/{date_str}")
def get_water_logs(date_str: str):
    files = get_csv_files_for_date("water", date_str)
    if not files:
        return []
    try:
        df = merge_csv_files(files)
        if df.empty:
            return []

        df['id'] = df.index

        # amount: duration_frames ê¸°ë°˜ìœ¼ë¡œ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
        if 'duration_frames' in df.columns and 'amount' not in df.columns:
            df['amount'] = pd.to_numeric(df['duration_frames'], errors='coerce').fillna(0).astype(int)
        elif 'amount' not in df.columns:
            df['amount'] = 0

        # â˜… timestamp ê¸°ì¤€ìœ¼ë¡œ captures ì´ë¯¸ì§€ ì°¾ê¸° (capture_pathëŠ” ë¬´ì‹œ)
        if 'timestamp' in df.columns:
            df['imageUrl'] = df['timestamp'].apply(
                lambda ts: find_capture_by_timestamp('water_drinking', ts)
            )
        else:
            df['imageUrl'] = None

        df['imageFile'] = df['imageUrl'].apply(
            lambda url: os.path.basename(str(url)) if isinstance(url, str) else None
        )

        if 'ai_result' not in df.columns:
            df['ai_result'] = "Not Analyzed"

        df = df.replace([float('inf'), float('-inf')], None)
        df = df.astype(object).where(pd.notnull(df), None)
        return df.to_dict(orient="records")
    except Exception as e:
        print("water ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        return []



@app.get("/api/logs/study/{date_str}")
def get_study_logs(date_str: str):
    files = get_csv_files_for_date("study", date_str)
    if not files:
        return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}
    try:
        df = merge_csv_files(files)
        if df.empty:
            return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}

        # duration_sec â†’ duration_min (ì—†ìœ¼ë©´ ìƒì„±)
        # duration_sec â†’ duration_min ë™ê¸°í™”
        # - CSVì—ì„œ duration_secë¥¼ ìˆ˜ì •í•˜ë©´ í•­ìƒ duration_minë„ í•¨ê»˜ ì—…ë°ì´íŠ¸ë˜ë„ë¡ ì²˜ë¦¬
        if 'duration_sec' in df.columns:
            df['duration_min'] = pd.to_numeric(df['duration_sec'], errors='coerce').fillna(0) / 60.0

        book_min, book_count, book_sessions = calculate_study_duration_per_file(df, 'book')
        laptop_min, laptop_count, laptop_sessions = calculate_study_duration_per_file(df, 'laptop')

        df['id'] = df.index

        # 1) ìš°ì„  CSVì˜ capture_pathë¡œë¶€í„° laptop/book ìº¡ì²˜ ì‚¬ìš©
        df['imageUrl'] = None
        if 'capture_path' in df.columns:
            df['imageUrl'] = df['capture_path'].apply(format_capture_url)

        # 2) ì—†ëŠ” ê²ƒë§Œ timestamp ê¸°ë°˜ study_start / study_end / study_* ì—ì„œ ì°¾ì•„ì˜¤ê¸°
        if 'timestamp' in df.columns:
            mask = df['imageUrl'].isna()
            df.loc[mask, 'imageUrl'] = df.loc[mask, 'timestamp'].apply(
                lambda ts: find_capture_by_timestamp(['study_start', 'study_end', 'study'], ts)
            )

        df['imageFile'] = df['imageUrl'].apply(
            lambda url: os.path.basename(str(url)) if isinstance(url, str) else None
        )

        if 'object' in df.columns:
            df['type'] = df['object'].apply(
                lambda x: str(x).lower() if pd.notna(x) else 'laptop'
            )

        if 'timestamp' in df.columns:
            df['time'] = df['timestamp'].apply(
                lambda x: str(x).split('T')[1][:5]
                if pd.notna(x) and 'T' in str(x)
                else (str(x).split(' ')[1][:5]
                      if pd.notna(x) and ' ' in str(x)
                      else None)
            )

        df = df.replace([float('inf'), float('-inf')], None)
        df = df.astype(object).where(pd.notnull(df), None)

        return {
            "logs": df.to_dict(orient="records"),
            "totalBookMin": book_min,
            "totalLaptopMin": laptop_min,
            "sessions": book_sessions + laptop_sessions,
            "activityCount": book_count + laptop_count,
        }
    except Exception as e:
        print("study ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}


def find_capture_by_timestamp(prefixes, timestamp: str) -> Optional[str]:
    """
    prefixes: 'water_drinking' ë˜ëŠ” ['study_start', 'study_end', 'study']ì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸
    timestamp: '2025-12-04T20:44:23' ë˜ëŠ” '2025-12-04 20:44:23'
    â†’ Desktop/captures/{prefix}_{YYYY-MM-DD}_{HH-MM}* ë¥¼ ì°¾ì•„ì„œ ì²« íŒŒì¼ì„ ë°˜í™˜
    """
    if not timestamp:
        return None
    try:
        dt = pd.to_datetime(str(timestamp))
    except Exception:
        return None

    date_str = dt.strftime('%Y-%m-%d')   # 2025-12-04
    time_key = dt.strftime('%H-%M')      # 20-44

    if isinstance(prefixes, str):
        prefixes = [prefixes]

    for p in prefixes:
        pattern = os.path.join(CAPTURES_DIR, f"{p}_{date_str}_{time_key}*")
        matches = glob.glob(pattern)
        if matches:
            return format_capture_url(matches[0])

    return None




# ======================================================================
# âœ… [ìµœì¢… ìˆ˜ì •] í”„ë¡ íŠ¸ì—”ë“œ ìš”ì²­(/api/logs/update)ì„ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ìˆ˜ì • API
# ======================================================================
@app.post("/api/logs/update")
def update_log_generic(payload: LogUpdateRequest):
    """
    í”„ë¡ íŠ¸ì—”ë“œ useStudyLogs.jsê°€ ë³´ë‚´ëŠ” ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    URL: /api/logs/update
    Payload: { source_file, log_id, updates: { key: value } }
    """
    print(f"ğŸ“¥ ë¡œê·¸ ì—…ë°ì´íŠ¸ ìš”ì²­: {payload.source_file} / log_id={payload.log_id}")
    
    try:
        # 1. íŒŒì¼ ì°¾ê¸°
        file_path = os.path.join(LOGS_DIR, payload.source_file)
        if not os.path.exists(file_path):
            search_pattern = os.path.join(LOGS_DIR, f"*{payload.source_file}*")
            candidates = glob.glob(search_pattern)
            if candidates:
                file_path = candidates[0]
            else:
                print(f"âŒ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {payload.source_file}")
                raise HTTPException(status_code=404, detail="File not found")

        # 2. CSV ì½ê¸°
        df = pd.read_csv(file_path)

        # 3. í‚¤ ë§¤í•‘
        #    - í”„ë¡ íŠ¸ëŠ” ë³´í†µ snake_case(book_title, read_pages)ë¥¼ ë³´ëƒ„
        #    - í˜¹ì‹œ camelCase(bookTitle, readPages)ê°€ ì™€ë„ snake_caseë¡œ ë§¤í•‘
        key_map = {
            "bookTitle": "book_title",
            "bookAuthors": "book_authors",
            "bookThumbnail": "book_thumbnail",
            "readPages": "read_pages",
            "totalPages": "total_pages",
            "durationMin": "duration_min",
        }

        updates = payload.updates or {}
        # íŒŒì¼ ì „ì²´ ì—…ë°ì´íŠ¸ëŠ” ëª…ì‹œì ìœ¼ë¡œ "all" ë˜ëŠ” ë¹ˆ ê°’ì¼ ë•Œë§Œ ì²˜ë¦¬
        # (0 ì€ ì‹¤ì œ ì²« ë²ˆì§¸ ë¡œê·¸ ì¸ë±ìŠ¤ë¡œ ì‚¬ìš©)
        is_file_wide_update = str(payload.log_id) in ("all", "")

        # 4. ì—…ë°ì´íŠ¸ ì ìš©
        for key, value in updates.items():
            # 4-1. ì»¬ëŸ¼ëª… ê²°ì • (ìš°ì„ : snake_case / ë³´ì¡°: key_map)
            col_name = key
            if col_name not in df.columns and key in key_map:
                col_name = key_map[key]

            if col_name not in df.columns:
                # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆì§€ë§Œ, ì¼ë‹¨ ê²½ê³ ë§Œ ì°ê³  ìŠ¤í‚µ
                print(f"âš ï¸ CSVì— '{col_name}' ì»¬ëŸ¼ì´ ì—†ì–´ ìŠ¤í‚µë¨. (ì›ë˜ í‚¤: {key})")
                continue

            if is_file_wide_update:
                # íŒŒì¼ ë‚´ ëª¨ë“  í–‰ ì—…ë°ì´íŠ¸ (ì±… ì •ë³´ ìˆ˜ì • ì‹œ)
                df.loc[:, col_name] = value
            else:
                # ê°œë³„ ë¡œê·¸ ìˆ˜ì • (manual update)
                try:
                    idx = int(payload.log_id)
                    if 0 <= idx < len(df):
                        df.at[idx, col_name] = value
                except ValueError:
                    print(f"âš ï¸ log_idê°€ ìˆ«ìê°€ ì•„ë‹˜: {payload.log_id}")
                    continue

        # 5. ì €ì¥
        df.to_csv(file_path, index=False, encoding="utf-8-sig")
        print("âœ… ë¡œê·¸ ì—…ë°ì´íŠ¸ ì €ì¥ ì™„ë£Œ:", file_path)
        
        return {"status": "success"}

    except HTTPException:
        raise
    except Exception as e:
        print("âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze")
async def analyze_image(request: AnalysisRequest):
    if VISION_MODEL is None:
        print("âš ï¸ VISION_MODEL ë¯¸ì´ˆê¸°í™”")
        return {"result": "Analysis Failed"}

    image_path = os.path.join(CAPTURES_DIR, request.image_filename)
    if not os.path.exists(image_path):
        raise HTTPException(status_code=404, detail="Image not found")
    
    try:
        img = genai.upload_file(path=image_path)
        prompt = "ì´ ì‚¬ì§„ ì† ìŒë£Œê°€ ë¬´ì—‡ì¸ì§€ í•œ ë‹¨ì–´ë¡œ ë§í•´ì¤˜(ì˜ˆ: ì½œë¼, ë¬¼, ì»¤í”¼). ì»µë§Œ ë³´ì´ë©´ ë¬¼."
        response = VISION_MODEL.generate_content([prompt, img])
        return {"result": response.text.strip()}
    except Exception as e:
        print("âŒ Image analysis error:", e)
        traceback.print_exc()
        return {"result": "Analysis Failed"}


@app.post("/api/summary")
async def generate_summary(request: SummaryRequest):
    if TEXT_MODEL is None:
        return {
            "summary": (
                "ì˜¤ëŠ˜ì€ ë¬¼ê³¼ ê³µë¶€ ê¸°ë¡ì„ ì°¨ë¶„íˆ ìŒ“ì•„ê°€ëŠ” í•˜ë£¨ì˜€ì–´ìš”. "
                "ë‚´ì¼ë„ ë„ˆë¬´ ë¬´ë¦¬í•˜ì§€ ë§ê³  ê¾¸ì¤€í•œ í˜ì´ìŠ¤ë¥¼ ì´ì–´ê°€ë©´ ì¢‹ê² ì–´ìš”."
            )
        }

    try:
        # ë¬¼/ê³µë¶€ ë‹¬ì„± ì—¬ë¶€
        water_achieved = "ë‹¬ì„±" if request.waterMl >= request.waterGoal else "ë¶€ì¡±"
        study_achieved = "ë‹¬ì„±" if request.studyMin >= request.studyGoal else "ë¶€ì¡±"

        # ê¸°ë³¸ ì •ë³´ ì •ë¦¬
        base_info = f"""
- ë¬¼ ì„­ì·¨: {request.waterMl}ml / ëª©í‘œ {request.waterGoal}ml ({water_achieved})
- ê³µë¶€: {request.studyMin}ë¶„ / ëª©í‘œ {request.studyGoal}ë¶„ ({study_achieved})
"""

        # ğŸ’» ë…¸íŠ¸ë¶ í™œë™ ìš”ì•½
        laptop_section = ""
        if request.laptopInfo and request.laptopInfo.durationMin > 0:
            laptop = request.laptopInfo
            category_names = {
                "lecture": "ê°•ì˜ ì‹œì²­",
                "assignment": "ê³¼ì œ",
                "coding": "ì½”ë”©",
                "youtube": "YouTube",
                "game": "ê²Œì„",
            }
            cat_name = category_names.get(laptop.category, laptop.category)
            laptop_section = f"- ë…¸íŠ¸ë¶ í™œë™: {cat_name} {laptop.durationMin}ë¶„\n"

        # ğŸ“š ì±… ì •ë³´
        book_section = ""
        if request.bookInfo and (request.bookInfo.title or request.bookInfo.description):
            book = request.bookInfo
            purpose_text = "í•™ìŠµ ëª©ì " if book.purpose == "study" else "ì·¨ë¯¸ ë…ì„œ"
            book_section = f"""
- ì˜¤ëŠ˜ ì½ì€ ì±…: "{book.title or 'ì œëª© ë¯¸ê¸°ë¡'}"
- ì €ì: {', '.join(book.authors) if book.authors else 'ë¯¸ìƒ'}
- ì½ì€ í˜ì´ì§€: {book.readPages}p / {book.totalPages}p
- ë…ì„œ ì‹œê°„: {book.durationMin}ë¶„
- ë…ì„œ ëª©ì : {purpose_text}
- ì±… ì„¤ëª…: {book.description[:200] if book.description else 'ì„¤ëª… ì—†ìŒ'}
"""

        # ğŸ“Œ í†µí•© í”„ë¡¬í”„íŠ¸ â€” í•˜ë£¨ ìš”ì•½ + ë¬¼ + ê³µë¶€ + ë…¸íŠ¸ë¶ + ë…ì„œ(ìˆìœ¼ë©´)
        prompt = f"""
ë‹¹ì‹ ì€ ì°¨ë¶„í•˜ê³  ë”°ëœ»í•œ í•˜ë£¨ ë¦¬í¬íŠ¸ ì½”ì¹˜ì…ë‹ˆë‹¤.

[ì˜¤ëŠ˜ì˜ ê¸°ë¡]
{base_info}{laptop_section}{book_section}

[ì‘ì„± ê·œì¹™]
1) ì²« 1~2ë¬¸ì¥ì€ ë¬¼ ì„­ì·¨ì™€ ê³µë¶€ ëª©í‘œ ë‹¬ì„± ì •ë„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ,
   "ì˜¤ëŠ˜ì€ ~í•œ í•˜ë£¨ì˜€ì–´ìš”" í˜•íƒœë¡œ í•˜ë£¨ ì „ì²´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•˜ì„¸ìš”.

2) ë‹¤ìŒ 1~2ë¬¸ì¥ì€ ë¬¼ ì„­ì·¨ ìŠµê´€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°±ê³¼
   ë‚´ì¼ì„ ìœ„í•œ í•œë‘ ê°€ì§€ í˜„ì‹¤ì ì¸ ì œì•ˆì„ ì ì–´ ì£¼ì„¸ìš”
   (ë„ˆë¬´ ê³¼ì¥ë˜ê±°ë‚˜ ëª…ë ¹ì¡°ì¸ í‘œí˜„ì€ í”¼í•˜ê¸°).

3) ê·¸ ë‹¤ìŒ 1~2ë¬¸ì¥ì€ ê³µë¶€/ì§‘ì¤‘ íŒ¨í„´ì— ëŒ€í•œ í”¼ë“œë°±ê³¼
   ëª©í‘œ ë‹¬ì„±ì— ë„ì›€ì´ ë  ì°¨ë¶„í•œ ì¡°ì–¸ì„ ì ì–´ ì£¼ì„¸ìš”.

4) ë§ˆì§€ë§‰ 1~2ë¬¸ì¥ì€ ì˜¤ëŠ˜ ì½ì€ ì±…ì´ ë§ˆìŒê³¼ í•˜ë£¨ì—
   ì–´ë–¤ ì—¬ìš´ì´ë‚˜ ì‘ì€ ë³€í™”ë¥¼ ë‚¨ê²¼ëŠ”ì§€ ìš”ì•½í•´ ì£¼ì„¸ìš”.
   ì¤„ê±°ë¦¬ ì„¤ëª…ì€ ìµœì†Œí™”í•˜ê³  ê°ì •Â·ì„±ì¥ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
   (ë§Œì•½ ë…ì„œ ê¸°ë¡ì´ ì—†ë‹¤ë©´ ì´ í•­ëª©ì€ ìì—°ìŠ¤ëŸ½ê²Œ ìƒëµí•˜ì„¸ìš”.)

5) ì „ì²´ëŠ” 5~7ë¬¸ì¥, ì¡´ëŒ“ë§, ì°¨ë¶„í•˜ì§€ë§Œ ë”°ëœ»í•œ í†¤.
   ì§€ë‚˜ì¹˜ê²Œ ê·¹ì ì¸ í‘œí˜„ì´ë‚˜ ê³¼ì¥ëœ ê²©ë ¤ëŠ” í”¼í•˜ì„¸ìš”.
"""

        # ğŸ”¥ AI ì‹¤í–‰
        response = TEXT_MODEL.generate_content(prompt)
        summary = ' '.join(response.text.strip().split())

        return {"summary": summary}

    except Exception as e:
        print("âŒ AI summary error:", e)
        traceback.print_exc()
        return {
            "summary": "ì˜¤ëŠ˜ì€ ë¬¼ê³¼ ê³µë¶€ ê¸°ë¡ì„ ì°¨ë¶„íˆ ìŒ“ì•„ê°€ëŠ” í•˜ë£¨ì˜€ì–´ìš”. ë‚´ì¼ë„ ë¬´ë¦¬í•˜ì§€ ë§ê³  í¸ì•ˆí•˜ê²Œ ì´ì–´ê°€ë³´ì„¸ìš”."
        }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
