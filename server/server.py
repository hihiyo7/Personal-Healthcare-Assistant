# íŒŒì¼ëª…: server.py
# ============================================================
# Personal Healthcare Assistant - Backend Server (Final Fixed)
# ============================================================

import os
from pathlib import Path
from dotenv import load_dotenv
import glob
import pandas as pd
from typing import Optional, List, Dict, Any
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import google.generativeai as genai
import uvicorn
import traceback
import re

# ==========================================
# .env ë¡œë“œ (ê²½ë¡œ ê³ ì •)
# ==========================================
BASE_DIR = Path(__file__).resolve().parent
ROOT_DIR = BASE_DIR.parent
ENV_PATH = ROOT_DIR / ".env"

load_dotenv(dotenv_path=ENV_PATH, override=True)

GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise RuntimeError("GOOGLE_API_KEY ë¡œë“œ ì‹¤íŒ¨")

# ==========================================
# Gemini ëª¨ë¸ ì„¤ì •
# ==========================================
genai.configure(api_key=GOOGLE_API_KEY)

try:
    TEXT_MODEL = genai.GenerativeModel("models/gemini-2.5-flash")
    VISION_MODEL = genai.GenerativeModel("models/gemini-2.5-flash-image")
    print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (2.5-flash)")
except Exception as e:
    print("âŒ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
    traceback.print_exc()
    TEXT_MODEL = None
    VISION_MODEL = None


# ==========================================
# ë°ì´í„° ê²½ë¡œ ì„¤ì •
# ==========================================
DATA_DIR = os.environ.get("DATA_DIR", r"C:/Users/gaeun/Desktop")
LOGS_DIR = os.path.join(DATA_DIR, "logs")
CAPTURES_DIR = os.path.join(DATA_DIR, "captures")

app = FastAPI()

# 422 ì—ëŸ¬ ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ í•¸ë“¤ëŸ¬
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    print(f"âŒ ë°ì´í„° ê²€ì¦ ì—ëŸ¬ ë°œìƒ: {exc}")
    return JSONResponse(status_code=422, content={"detail": exc.errors()})

app.add_middleware(
    CORSMIDDLEWARE := CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if os.path.exists(CAPTURES_DIR):
    app.mount("/captures", StaticFiles(directory=CAPTURES_DIR), name="captures")


@app.get("/")
def read_root():
    return {"status": "Server running", "data_path": DATA_DIR}


# ==========================================
# Pydantic ëª¨ë¸
# ==========================================
class AnalysisRequest(BaseModel):
    image_filename: str
    log_id: int

class BookInfo(BaseModel):
    title: str = ""
    authors: List[str] = []
    readPages: int = 0
    totalPages: int = 0
    durationMin: float = 0.0
    description: str = ""
    purpose: str = "study"

class LaptopInfo(BaseModel):
    category: str = "lecture"
    durationMin: float = 0.0
    isStudy: bool = True

class SummaryRequest(BaseModel):
    date: str
    waterMl: float
    waterGoal: float
    studyMin: float
    studyGoal: float
    bookInfo: Optional[BookInfo] = None
    laptopInfo: Optional[LaptopInfo] = None

# í”„ë¡ íŠ¸ì—”ë“œ useStudyLogs.jsì™€ ì¼ì¹˜í•˜ëŠ” ëª¨ë¸
class LogUpdateRequest(BaseModel):
    source_file: str            # í”„ë¡ íŠ¸ì—”ë“œê°€ ë³´ë‚´ëŠ” íŒŒì¼ëª…
    log_id: int | str           # ë¡œê·¸ ID (0ì´ë©´ ì „ì²´ ìˆ˜ì •)
    updates: Dict[str, Any]     # ë³€ê²½í•  ë°ì´í„° { "book_title": "...", ... }

# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================================
def get_csv_files_for_date(prefix: str, date_str: str) -> list:
    pattern = os.path.join(LOGS_DIR, f"{prefix}*{date_str}*.csv")
    return list(set(glob.glob(pattern)))

def parse_timestamp_from_filename(filename: str) -> Optional[str]:
    basename = os.path.basename(filename).replace('.csv', '')
    match1 = re.search(r'(\d{4}-\d{2}-\d{2})-(\d{2})-(\d{2})', basename)
    if match1:
        return f"{match1.group(1)}T{match1.group(2)}:{match1.group(3)}:00"
    match2 = re.search(r'(\d{4}-\d{2}-\d{2})_(\d{2}-\d{2}-\d{2})', basename)
    if match2:
        return f"{match2.group(1)}T{match2.group(2).replace('-', ':')}"
    return None

def merge_csv_files(files: list) -> pd.DataFrame:
    if not files:
        return pd.DataFrame()
    dfs = []
    for f in files:
        try:
            df = pd.read_csv(f)
            if not df.empty:
                file_ts = parse_timestamp_from_filename(f)
                if file_ts and 'timestamp' not in df.columns:
                    df['timestamp'] = file_ts
                df['source_file'] = os.path.basename(f)
                dfs.append(df)
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {f} / {e}")
            traceback.print_exc()
            continue
    if not dfs:
        return pd.DataFrame()
    
    merged = pd.concat(dfs, ignore_index=True)
    if 'timestamp' in merged.columns:
        merged = merged.sort_values('timestamp').reset_index(drop=True)
    return merged

def format_capture_url(path):
    if pd.isna(path) or str(path).lower() == 'nan' or 'Started' in str(path):
        return None
    return f"http://localhost:8000/captures/{os.path.basename(str(path))}"

def calculate_study_duration_per_file(df: pd.DataFrame, obj_type: str):
    if 'object' not in df.columns or 'source_file' not in df.columns:
        return 0, 0, []

    type_df = df[df['object'].astype(str).str.lower() == obj_type.lower()]
    if type_df.empty or 'timestamp' not in type_df.columns:
        return 0, 0, []

    total_min = 0
    sessions = []

    for source_file, group in type_df.groupby('source_file'):
        try:
            timestamps = group['timestamp'].dropna().tolist()
            if len(timestamps) < 1:
                continue
            first = pd.to_datetime(timestamps[0])
            last = pd.to_datetime(timestamps[-1])
            duration = max(int((last - first).total_seconds() / 60), 1)
            total_min += duration

            sessions.append({
                'source_file': source_file,
                'start_time': first.strftime('%H:%M'),
                'end_time': last.strftime('%H:%M'),
                'duration_min': duration,
                'log_count': len(timestamps),
            })
        except Exception as e:
            print(f"âŒ duration ê³„ì‚° ì‹¤íŒ¨ ({source_file}): {e}")
            traceback.print_exc()
            continue

    return total_min, len(sessions), sessions


# ==========================================
# API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/api/logs/water/{date_str}")
def get_water_logs(date_str: str):
    files = get_csv_files_for_date("water", date_str)
    if not files:
        return []
    try:
        df = merge_csv_files(files)
        if df.empty:
            return []
        df['id'] = df.index

        if 'duration_frames' in df.columns:
            df['amount'] = pd.to_numeric(df['duration_frames'], errors='coerce').fillna(200).astype(int)
        else:
            df['amount'] = 200

        if 'capture_path' in df.columns:
            df['imageUrl'] = df['capture_path'].apply(format_capture_url)
            df['imageFile'] = df['capture_path'].apply(
                lambda x: os.path.basename(str(x)) if isinstance(x, str) else None
            )

        if 'ai_result' not in df.columns:
            df['ai_result'] = "Not Analyzed"

        df = df.replace([float('inf'), float('-inf')], None)
        df = df.astype(object).where(pd.notnull(df), None)
        return df.to_dict(orient="records")
    except Exception as e:
        print("âŒ water ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        return []


@app.get("/api/logs/study/{date_str}")
def get_study_logs(date_str: str):
    files = get_csv_files_for_date("study", date_str)
    if not files:
        return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}
    try:
        df = merge_csv_files(files)
        if df.empty:
            return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}

        book_min, book_count, book_sessions = calculate_study_duration_per_file(df, 'book')
        laptop_min, laptop_count, laptop_sessions = calculate_study_duration_per_file(df, 'laptop')

        df['id'] = df.index
        if 'capture_path' in df.columns:
            df['imageUrl'] = df['capture_path'].apply(format_capture_url)
            df['imageFile'] = df['capture_path'].apply(
                lambda x: os.path.basename(str(x)) if isinstance(x, str) else None
            )

        if 'object' in df.columns:
            df['type'] = df['object'].apply(
                lambda x: str(x).lower() if pd.notna(x) else 'laptop'
            )

        if 'timestamp' in df.columns:
            df['time'] = df['timestamp'].apply(
                lambda x: str(x).split('T')[1][:5]
                if pd.notna(x) and 'T' in str(x)
                else (str(x).split(' ')[1][:5]
                      if pd.notna(x) and ' ' in str(x)
                      else None)
            )

        df = df.replace([float('inf'), float('-inf')], None)
        df = df.astype(object).where(pd.notnull(df), None)

        return {
            "logs": df.to_dict(orient="records"),
            "totalBookMin": book_min,
            "totalLaptopMin": laptop_min,
            "sessions": book_sessions + laptop_sessions,
            "activityCount": book_count + laptop_count,
        }
    except Exception as e:
        print("âŒ study ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨:", e)
        traceback.print_exc()
        return {"logs": [], "totalBookMin": 0, "totalLaptopMin": 0, "sessions": []}


# ======================================================================
# âœ… [ìµœì¢… ìˆ˜ì •] í”„ë¡ íŠ¸ì—”ë“œ ìš”ì²­(/api/logs/update)ì„ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ìˆ˜ì • API
# ======================================================================
@app.post("/api/logs/update")
def update_log_generic(payload: LogUpdateRequest):
    """
    í”„ë¡ íŠ¸ì—”ë“œ useStudyLogs.jsê°€ ë³´ë‚´ëŠ” ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    URL: /api/logs/update
    Payload: { source_file, log_id, updates: { key: value } }
    """
    print(f"ğŸ“¥ ë¡œê·¸ ì—…ë°ì´íŠ¸ ìš”ì²­: {payload.source_file} / log_id={payload.log_id}")
    
    try:
        # 1. íŒŒì¼ ì°¾ê¸°
        file_path = os.path.join(LOGS_DIR, payload.source_file)
        if not os.path.exists(file_path):
            search_pattern = os.path.join(LOGS_DIR, f"*{payload.source_file}*")
            candidates = glob.glob(search_pattern)
            if candidates:
                file_path = candidates[0]
            else:
                print(f"âŒ íŒŒì¼ ì°¾ê¸° ì‹¤íŒ¨: {payload.source_file}")
                raise HTTPException(status_code=404, detail="File not found")

        # 2. CSV ì½ê¸°
        df = pd.read_csv(file_path)

        # 3. í‚¤ ë§¤í•‘
        #    - í”„ë¡ íŠ¸ëŠ” ë³´í†µ snake_case(book_title, read_pages)ë¥¼ ë³´ëƒ„
        #    - í˜¹ì‹œ camelCase(bookTitle, readPages)ê°€ ì™€ë„ snake_caseë¡œ ë§¤í•‘
        key_map = {
            "bookTitle": "book_title",
            "bookAuthors": "book_authors",
            "bookThumbnail": "book_thumbnail",
            "readPages": "read_pages",
            "totalPages": "total_pages",
            "durationMin": "duration_min",
        }

        updates = payload.updates or {}
        is_file_wide_update = str(payload.log_id) in ("0", "all", "")

        # 4. ì—…ë°ì´íŠ¸ ì ìš©
        for key, value in updates.items():
            # 4-1. ì»¬ëŸ¼ëª… ê²°ì • (ìš°ì„ : snake_case / ë³´ì¡°: key_map)
            col_name = key
            if col_name not in df.columns and key in key_map:
                col_name = key_map[key]

            if col_name not in df.columns:
                # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆì§€ë§Œ, ì¼ë‹¨ ê²½ê³ ë§Œ ì°ê³  ìŠ¤í‚µ
                print(f"âš ï¸ CSVì— '{col_name}' ì»¬ëŸ¼ì´ ì—†ì–´ ìŠ¤í‚µë¨. (ì›ë˜ í‚¤: {key})")
                continue

            if is_file_wide_update:
                # íŒŒì¼ ë‚´ ëª¨ë“  í–‰ ì—…ë°ì´íŠ¸ (ì±… ì •ë³´ ìˆ˜ì • ì‹œ)
                df.loc[:, col_name] = value
            else:
                # ê°œë³„ ë¡œê·¸ ìˆ˜ì • (manual update)
                try:
                    idx = int(payload.log_id)
                    if 0 <= idx < len(df):
                        df.at[idx, col_name] = value
                except ValueError:
                    print(f"âš ï¸ log_idê°€ ìˆ«ìê°€ ì•„ë‹˜: {payload.log_id}")
                    continue

        # 5. ì €ì¥
        df.to_csv(file_path, index=False, encoding="utf-8-sig")
        print("âœ… ë¡œê·¸ ì—…ë°ì´íŠ¸ ì €ì¥ ì™„ë£Œ:", file_path)
        
        return {"status": "success"}

    except HTTPException:
        raise
    except Exception as e:
        print("âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze")
async def analyze_image(request: AnalysisRequest):
    if VISION_MODEL is None:
        print("âš ï¸ VISION_MODEL ë¯¸ì´ˆê¸°í™”")
        return {"result": "Analysis Failed"}

    image_path = os.path.join(CAPTURES_DIR, request.image_filename)
    if not os.path.exists(image_path):
        raise HTTPException(status_code=404, detail="Image not found")
    
    try:
        img = genai.upload_file(path=image_path)
        prompt = "ì´ ì‚¬ì§„ ì† ìŒë£Œê°€ ë¬´ì—‡ì¸ì§€ í•œ ë‹¨ì–´ë¡œ ë§í•´ì¤˜(ì˜ˆ: ì½œë¼, ë¬¼, ì»¤í”¼). ì»µë§Œ ë³´ì´ë©´ ë¬¼."
        response = VISION_MODEL.generate_content([prompt, img])
        return {"result": response.text.strip()}
    except Exception as e:
        print("âŒ Image analysis error:", e)
        traceback.print_exc()
        return {"result": "Analysis Failed"}


@app.post("/api/summary")
async def generate_summary(request: SummaryRequest):
    if TEXT_MODEL is None:
        return {"summary": "ì˜¤ëŠ˜ì€ ë¬¼ê³¼ ê³µë¶€ ê¸°ë¡ì„ ì²œì²œíˆ ìŒ“ì•„ê°€ëŠ” í•˜ë£¨ì˜€ì–´ìš”. ë‚´ì¼ë„ ê±´ê°•í•œ ìŠµê´€ì„ ì´ì–´ê°€ë´ìš”."}

    try:
        water_achieved = "ë‹¬ì„±" if request.waterMl >= request.waterGoal else "ë¶€ì¡±"
        study_achieved = "ë‹¬ì„±" if request.studyMin >= request.studyGoal else "ë¶€ì¡±"
        
        base_info = f"""
- ë¬¼: {request.waterMl}ml / ëª©í‘œ {request.waterGoal}ml ({water_achieved})
- ê³µë¶€: {request.studyMin}ë¶„ / ëª©í‘œ {request.studyGoal}ë¶„ ({study_achieved})
"""

        book_section = ""
        if request.bookInfo and (request.bookInfo.title.strip() or request.bookInfo.description.strip()):
            book = request.bookInfo
            purpose_text = "í•™ìŠµ ëª©ì " if book.purpose == "study" else "ì·¨ë¯¸ ë…ì„œ"
            book_section = f"""
- ì˜¤ëŠ˜ ì½ì€ ì±…: "{book.title or 'ì œëª© ë¯¸ê¸°ë¡'}"
- ì €ì: {', '.join(book.authors) if book.authors else 'ë¯¸ìƒ'}
- ì½ì€ í˜ì´ì§€: {book.readPages}p / {book.totalPages}p
- ë…ì„œ ì‹œê°„: {book.durationMin}ë¶„
- ë…ì„œ ëª©ì : {purpose_text}
- ì±… ì„¤ëª…: {book.description[:200] if book.description else 'ì„¤ëª… ì—†ìŒ'}
"""
        
        laptop_section = ""
        if request.laptopInfo and request.laptopInfo.durationMin > 0:
            laptop = request.laptopInfo
            category_names = {
                'lecture': 'ê°•ì˜ ì‹œì²­',
                'assignment': 'ê³¼ì œ',
                'coding': 'ì½”ë”©',
                'youtube': 'YouTube',
                'game': 'ê²Œì„'
            }
            cat_name = category_names.get(laptop.category, laptop.category)
            laptop_section = f"""- ë…¸íŠ¸ë¶ í™œë™: {cat_name} ({laptop.durationMin}ë¶„)"""

        if book_section:
            prompt = f"""
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì§€ì ì¸ ë…ì„œ ì½”ì¹˜ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ê¸°ë¡:
{base_info}{book_section}{laptop_section}
ìš”êµ¬ì‚¬í•­:
1. "ì˜¤ëŠ˜ì€ ~í•œ í•˜ë£¨ì˜€ì–´ìš”"ë¡œ ì‹œì‘ (1-2ë¬¸ì¥)
2. ì±…ì˜ ì¤„ê±°ë¦¬ë‚˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ì–¸ê¸‰ (1-2ë¬¸ì¥)
3. ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì¡°ì–¸ì´ë‚˜ í†µì°° (1ë¬¸ì¥)
4. ë”°ëœ»í•œ ê²©ë ¤ë¡œ ë§ˆë¬´ë¦¬ (1ë¬¸ì¥)
5. ì´ 5~6ë¬¸ì¥ ì •ë„ì˜ í•œ ë‹¨ë½ìœ¼ë¡œ ì‘ì„±, ë¶ˆë¦¿ ê¸ˆì§€
"""
        else:
            prompt = f"""
ë‹¹ì‹ ì€ í•˜ë£¨ ìš”ì•½ ì½”ì¹˜ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ê¸°ë¡:
{base_info}{laptop_section}
ìš”êµ¬ì‚¬í•­:
- 2~3ë¬¸ì¥ í•œ ë‹¨ë½
- "ì˜¤ëŠ˜ì€ ~í•œ í•˜ë£¨ì˜€ì–´ìš”"ë¡œ ì‹œì‘
- ë¬¼/ê³µë¶€ ë‹¬ì„± ì—¬ë¶€ ì–¸ê¸‰
- ê²©ë ¤ í¬í•¨, ë¶ˆë¦¿ ê¸ˆì§€
"""

        response = TEXT_MODEL.generate_content(prompt)
        summary = ' '.join(response.text.strip().split())
        return {"summary": summary}
        
    except Exception as e:
        print("âŒ Gemini API error ìƒì„¸:", e)
        traceback.print_exc()
        return {"summary": "ì˜¤ëŠ˜ì€ ë¬¼ê³¼ ê³µë¶€ ê¸°ë¡ì„ ì²œì²œíˆ ìŒ“ì•„ê°€ëŠ” í•˜ë£¨ì˜€ì–´ìš”. ë‚´ì¼ë„ ê±´ê°•í•œ ìŠµê´€ì„ ì´ì–´ê°€ë´ìš”."}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
