# Personal Healthcare Assistant  
**First-Person Hand–Object Interaction 기반 일상 행동 모니터링 시스템**

---

## 1. 프로젝트 개요

본 프로젝트는 **1인칭 시점(First-Person View) 영상**을 기반으로 사용자의 일상 행동을 자동으로 기록·분석하고, 이를 통해 **예방적 개인 헬스케어 관리**를 지원하는 시스템이다.

기존 헬스케어 AI 연구는 특정 환자군이나 병원·재활 환경에 집중되어 있어, 건강한 일반 사용자의 일상 행동을 지속적으로 기록하고 개선으로 연결하는 데 한계가 있었다.  
이에 본 프로젝트는 **웨어러블 센서 없이 단일 카메라 입력만으로** 수분 섭취, 학습 활동과 같은 일상 행동을 자연스럽게 인식하고, 그 결과를 사용자 친화적인 UI로 해석·시각화하는 것을 목표로 한다.

---

## 2. 시스템 전체 파이프라인

본 시스템은 다음의 4단계 파이프라인으로 구성된다.

1. **On-device 실시간 감지**
   - YOLO-World를 이용한 객체 인식 (What)
   - MediaPipe Hands를 이용한 손 동작 추적 (How)

2. **Trigger 기반 캡처**
   - 의미 있는 행동의 시작/종료 시점만 선택적으로 이미지 캡처

3. **행동 분류 및 로그 기록**
   - 객체 종류 + 손 동작 패턴 + 지속시간을 결합한 규칙 기반 행동 판별
   - CSV 형태의 세션 단위 로그 저장

4. **데이터 분석 및 UI 시각화**
   - 일/주/월 단위 활동 요약
   - 목표 대비 달성도 및 행동 패턴 분석

본 README에서는 **Vision 기반 행동 인식 모델(1~3단계)**과  
**행동 로그를 해석·피드백으로 연결하는 UI 설계(4단계)**를 함께 설명한다.

---

## 3. Vision 기반 행동 인식 모델

### 3.1 설계 배경 및 접근 방식

기존 웨어러블 기반 시스템은 손의 움직임은 감지할 수 있으나, **어떤 객체를 사용했는지**에 대한 정보는 제공하지 못한다.  
이를 보완하기 위해 본 프로젝트는 **First-Person Hand–Object Interaction(HOI)** 분석 방식을 채택하였다.

본 모델은 다음 두 가지 질문에 답하는 구조로 설계되었다.

- **무엇을 사용하고 있는가?** → 객체 인식
- **어떻게 사용하고 있는가?** → 손 동작 및 제스처 분석

---

### 3.2 객체 인식 (YOLO-World)

- **모델**: YOLO-World (YOLOv8 기반)
- **Custom Class Set**:
  - 수분 섭취: cup, mug, water bottle, glass
  - 학습 활동: book, notebook, laptop, tablet, pen, paper 등

현실 환경에서의 오탐지를 줄이기 위해 다음 조건을 적용하였다.

- Confidence threshold
- Bounding box 크기 비율
- Aspect ratio 필터링

이를 통해 배경 객체나 우연한 탐지를 효과적으로 제거한다.

---

### 3.3 손 동작 추적 및 제스처 인식 (MediaPipe Hands)

- MediaPipe Hands를 사용해 **21개 손 랜드마크**를 실시간 추적
- 손바닥 중심(palm center)을 기준으로 객체와의 거리 및 이동 방향 계산

제스처 휴리스틱:
- 컵 파지(C-shape)
- 펜 집기(pinch gesture)

이 정보를 객체 인식 결과와 결합하여 행동 맥락을 판단한다.

---

### 3.4 Kalman Filter 기반 손 위치 복원

실사용 환경에서는 손이 객체에 가려지거나 카메라 시야를 벗어나는 경우가 빈번하다.  
이를 보완하기 위해 **2차원 Kalman Filter 기반 손 위치 추적 및 복원 로직**을 적용하였다.

- 상태 벡터: (x, y, vx, vy)
- 손이 일시적으로 사라진 경우:
  - 이전 움직임을 기반으로 위치 예측
  - 허용 프레임 내에서는 행동 추적 유지
- 허용 범위를 초과하면 행동 판별 중단

이를 통해 행동 인식의 연속성과 안정성을 확보하였다.

---

### 3.5 규칙 기반 행동 분류

행동은 단일 프레임이 아닌 **시간 누적 패턴**을 기준으로 판별된다.

#### 물 마시기
- 손–컵 거리 지속 접촉
- 손의 상방 이동량
- 이동 일관성
- 컵 파지 제스처 비율
- 쿨다운 로직으로 중복 기록 방지

#### 학습 활동
- 학습 관련 객체와의 지속적 상호작용
- 세션 단위 시작/종료 판별
- Book / Laptop / Tool 유형 분류

---

### 3.6 로그 저장 및 개인정보 고려

- **CSV 로그**
  - Timestamp, Action, Object, Duration 등 기록
- **이미지 캡처**
  - 행동 시작/종료 시점만 선택적으로 저장
  - 연속 영상 저장 없음

이미지는 행동 판별 근거로만 사용되며, UI 해석 이후 추가 활용은 제한된다.

---

## 4. UI 설계 및 구현

### 4.1 UI의 역할 정의 및 전체 구조

Vision 파이프라인이 생성하는 CSV 로그는 일반 사용자에게 바로 이해하기 어려운 원자료이다.  
이에 본 프로젝트에서는 Dashboard UI를  
**“행동 인식 AI 결과를 사용자 행동 개선으로 연결하는 데이터 해석 인터페이스”**로 정의하였다.

UI는 다음의 흐름을 제공한다.

- 하루 단위 요약
- 주·히스토리 비교
- 세부 화면 탐색

이를 통해 Vision 로그를 **이해 가능한 건강 지표**로 재구성한다.

---

### 4.2 행동 로그 → UI 데이터 변환 구조

UI는 CSV 원본을 직접 처리하지 않고, 상위 파이프라인에서 정리된 집계 결과를 전달받는다.

- `stats`: 수분 섭취량 등 일일 건강 지표
- `studySummary`: 총 공부 시간, 매체별 공부/비공부 시간

이를 통해 UI는 데이터 해석과 시각화에 집중하며, 기록 불일치를 방지한다.

---

### 4.3 상태 관리 및 세션 단위 기록 설계

- 현재 날짜의 실시간 집계와 과거 히스토리 분리
- 미래 날짜 업로드 차단
- 날짜 초기화 기능을 통한 사용자 통제

자동 기록의 한계를 UI 차원에서 보완하는 구조를 채택하였다.

---

### 4.4 주요 UI 구성

- **Dashboard**: 목표 달성 현황 및 종합 점수 제공
- **Study Activity**: 공부/비공부 시간 분리 시각화
- **Water Intake**: 목표 대비 섭취량 퍼센트 제공
- **Profile**: 사용자 목표 및 중·장기 행동 패턴 분석

---

### 4.5 창의적 설계 및 문제 해결

- 세션 단위 로그 기반 집계로 중복·과대평가 최소화
- AI 결과를 행동 조언으로 변환하여 실천 가능성 강화
- UI 레벨 입력 제어로 데이터 정합성 확보

---

### 4.6 UI 설계의 연구적 의의

본 UI는 행동 인식 AI의 불확실성을 흡수하여  
**자동 기록 + 사용자 통제**의 균형을 제공하는 인터페이스로 설계되었다.

이를 통해 행동 인식 결과를 단순 모니터링이 아닌  
**예방적 자기관리 및 행동 개선을 유도하는 디지털 헬스케어 인터페이스**로 확장하였다.

---

## 5. 정리

본 프로젝트는  
First-Person Hand–Object Interaction 기반 행동 인식을  
사용자가 이해하고 행동 개선으로 이어갈 수 있는 UI로 연결한  
**일상 헬스케어 보조 시스템**이다.

Vision 모델과 UI의 명확한 역할 분리를 통해  
실사용 환경에서의 안정성과 확장성을 동시에 확보하였다.
